---
title: QNN开发之QNN调试工具
date: 2020-07-20 00:00:00
updated: 2022-01-05 00:00:00
tags: [Linux,四大组件,Activity]
type: [Linux,四大组件,Activity]
comments: Activity的生命周期完全解析
description: Activity的生命周期完全解析
keywords: Activity的生命周期完全解析
top_img:
mathjax:
katex:
aside:
aplayer:
highlight_shrink:
---



[TOC]



# 概述

本文主要介绍QNN各种SDK工具和功能。



# qnn-net-run

qnn-net-run工具用于运行一个从QNN转换器的输出中编译的模型库，并在一个特定的后端运行。

```
描述:
------------
示范如何使用QNN APIs加载和执行神经网络的示例应用程序。


REQUIRED ARGUMENTS:
-------------------
  --model             <FILE>      指定模型
  --backend           <FILE>       Path to a QNN backend to execute the model.
  --input_list        <FILE>      文件目录：列出网络的输入。 如果model.so中有多个图，这必须是以逗号分隔的输入列表文件的清单。
  --retrieve_context  <VAL>       Path to cached binary from which to load a saved
                                  context from and execute graphs. --retrieve_context and
                                  --model are mutually exclusive. Only one of the options
                                  can be specified at a time.


OPTIONAL ARGUMENTS:
-------------------
  --model_prefix                  Function prefix to use when loading <qnn_model_name.so>.
                                  Default: QnnModel

  --debug                         Specifies that output from all layers of the network
                                  will be saved. This option can not be used when loading
                                  a saved context through --retrieve_context option.

  --output_dir        <DIR>       The directory to save output to. Defaults to ./output.

  --output_data_type  <VAL>       Data type of the output. Values can be:

                                    1. float_only:       dump outputs in float only.
                                    2. native_only:      dump outputs in data type native
                                                         to the model. For ex., uint8_t.
                                    3. float_and_native: dump outputs in both float and
                                                         native.

                                    (This is N/A for a float model. In other cases,
                                     if not specified, defaults to float_only.)

  --input_data_type   <VAL>       Data type of the input. Values can be:

                                    1. float:     reads inputs as floats and quantizes
                                                  if necessary based on quantization
                                                  parameters in the model.
                                    2. native:    reads inputs assuming the data type to be
                                                  native to the model. For ex., uint8_t.

                                    (This is N/A for a float model. In other cases,
                                     if not specified, defaults to float.)

  --op_packages       <VAL>       Provide a comma-separated list of op packages, interface
                                  providers, and, optionally, targets to register. Valid values
                                  for target are CPU and HTP. The syntax is:
                                  op_package_path:interface_provider:target[,op_package_path:interface_provider:target...]

  --profiling_level   <VAL>       Enable profiling. Valid Values:
                                    1. basic:    captures execution and init time.
                                    2. detailed: in addition to basic, captures per Op timing
                                                 for execution, if a backend supports it.

  --perf_profile       <VAL>      Specifies perf profile to set. Valid settings are
                                  low_balanced, balanced, default, high_performance,
                                  sustained_high_performance, burst, low_power_saver,
                                  power_saver, high_power_saver, and system_settings.

  --config_file        <FILE>     Path to a JSON config file. The config file currently
                                  supports options related to backend extensions,
                                  context priority and graph configs. Please refer to SDK
                                  documentation for more details.

  --log_level          <VAL>      Specifies max logging level to be set. Valid settings:
                                  error, warn, info, debug, and verbose.

  --shared_buffer                 Specifies creation of shared buffers for graph I/O between the application
                                  and the device/coprocessor associated with a backend directly.
                                  This option is currently supported on Android only.

  --version                       Print the QNN SDK version.

  --help                          Show this help message.
```





## qnn-model-lib-generator

qnn-model-lib-generator工具将QNN模型源代码编译成特定目标的工件。

```
usage: qnn-model-lib-generator [-h] [-c <QNN_MODEL>.cpp] [-b <QNN_MODEL>.bin]
       [-t LIB_TARGETS ] [-l LIB_NAME] [-o OUTPUT_DIR]
Script compiles provided Qnn Model artifacts for specified targets.

Required argument(s):
 -c <QNN_MODEL>.cpp                    Filepath for the qnn model .cpp file

optional argument(s):
 -b <QNN_MODEL>.bin                    Filepath for the qnn model .bin file
                                       (Note: if not passed, runtime will fail if .cpp needs any items from a .bin file.)

 -t LIB_TARGETS                        Specifies the targets to build the models for. Default: aarch64-android x86_64-linux-clang arm-android
 -l LIB_NAME                           Specifies the name to use for libraries. Default: uses name in <model.bin> if provided,
                                       else generic qnn_model.so
  -o OUTPUT_DIR                         Location for saving output libraries.
```













































