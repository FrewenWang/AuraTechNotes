---
title: ONNX开发环境搭建
date: 2019-09-20 00:00:00
updated: 2019-09-20 00:00:00
tags: [人工智能,机器学习,神经网络]
type: [人工智能,机器学习,神经网络]
comments:
description: 页面描述
keywords: 关键字
top_img:  页面顶部的图片
mathjax:
katex:
aside:
aplayer:
highlight_shrink: 配置代碼框是否展開(true/false)(默認為設置中highlight_shrink的配置)
---

[TOC]

文章参考：https://github.com/onnx/onnx

# 概述

# 环境搭建

## PyPi

```
pip install onnx -i https://pypi.tuna.tsinghua.edu.cn/simple
```



### Conda packages

A binary build of ONNX is available from [Conda](https://conda.io/), in [conda-forge](https://conda-forge.org/):

```shell
conda install -c conda-forge numpy protobuf==3.16.0 libprotobuf=3.16.0
conda install -c conda-forge onnx
```

You can also use the [onnx-dev docker image](https://hub.docker.com/r/onnx/onnx-dev) for a Linux-based installation without having to worry about dependency versioning.















## ONNX Runtime环境搭建

文章参考：https://onnxruntime.ai/



python版本环境搭建

```
pip install onnxruntime  -i  https://pypi.tuna.tsinghua.edu.cn/simple

# 可以指定版本号
(py36) frewen@freweniubuntu:~$ pip install onnxruntime==1.10.0  -i https://pypi.tuna.tsinghua.edu.cn/simple
```





### Mac下C++的ONNX Runtime环境搭建

文章参考：https://juejin.cn/post/7120494957625868325

文章参考：https://blog.csdn.net/qq_44747572/article/details/121467657?spm=1001.2014.3001.5502

文章参考：https://blog.csdn.net/qq_44747572/article/details/120820964?spm=1001.2014.3001.5501

文章参考：https://github.com/luogantt/onnxruntime_cpp_demo



下载对应的依赖库

https://onnxruntime.ai/

https://github.com/microsoft/onnxruntime/releases

![image-20230323225929717](./images/01.ONNX%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/image-20230323225929717.png)

集成到我们项目工程中

![image-20230323230337035](./images/01.ONNX%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/image-20230323230337035.png)

添加dependence依赖

```cmake
if (BUILD_ONNX_LIB)
    # 添加ONNX推理库的依赖
    # 参考文档: https://github.com/microsoft/onnxruntime/releases/tag/v1.14.1
    set(ONNX_VERSION v1.14.1)  # 设置QNN的版本号
    set(ONNX_BASE_DIR ${LIB_DIR}/onnx/${ONNX_VERSION}/${TARGET_OS}-${TARGET_ARCH}-release)   # 设置QNN的基础目录
    set(ONNX_INCLUDE_DIR ${ONNX_BASE_DIR}/include)
    set(ONNX_LINK_DIR ${ONNX_BASE_DIR}/lib)   # 根据系统版本设置依赖库的目录
    set(ONNX_LIB onnxruntime)
    include_directories(${ONNX_INCLUDE_DIR})
    link_directories(${ONNX_LINK_DIR})
    message(STATUS "[Dependency] onnx ONNX_INCLUDE_DIR = ${ONNX_INCLUDE_DIR}")
    message(STATUS "[Dependency] onnx ONNX_LINK_DIR = ${ONNX_LINK_DIR}")
    message(STATUS "[Dependency] onnx ONNX_LIB = ${ONNX_LIB}")
else ()
    message(STATUS "[Dependency] NotBuild onnx dependence ignored")
endif ()
```

参考集成的实例库

https://github.com/microsoft/onnxruntime-inference-examples



# 源码实现

```c++
class OnnxPredictor : public AbsPredictor {
public:
    ~OnnxPredictor() override = default;

    int init(const ModelInfo &model) override;

    int doPredict(aura_cv::TensorArray &inputs, aura_cv::TensorArray &outputs) override;

    int deInit() override;

private:
    /**
     * 定义OnnxRuntime Session
     */
    Ort::Session session;
    /**
     * 定义OnnxRuntime MemoryInfo
     */
    Ort::MemoryInfo memoryInfo;
    /**
     * 输入张量的大小
     */
    size_t inputTensorSize;
    /**
     * 输入张量的数据
     */
    std::vector<float> inputTensorValues;

    std::vector<const char *> input_node_names = {"input", "input_mask"};
    std::vector<const char *> output_node_names = {"output", "output_mask"};
    std::vector<int64_t> input_node_dims = {10, 20};

};
```





```
int OnnxPredictor::init(const ModelInfo &model) {
    if (model.blobs.size() != 1) {
        ALOGE(TAG, "Predictor model mem error, one blob needed! error model info:%s", model.version.c_str());
        VA_RET(Error::MODEL_INIT_ERR);
    }
    auto *modelData = static_cast<const uint8_t *>(model.blobs[0].data);
    auto modelSize = static_cast<const size_t>(model.blobs[0].len);

    // onnxruntime setup onnxruntime的初始化
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "example-model-explorer");
    // 获取配置
    Ort::SessionOptions session_options;
    // 使用1个线程执行op,若想提升速度，增加线程数
    session_options.SetIntraOpNumThreads(1);

    // CUDA加速开启(由于onnxruntime的版本太高，无cuda_provider_factory.h的头文件，加速可以使用onnxruntime V1.8的版本)
    // OrtSessionOptionsAppendExecutionProvider_CUDA(session_options, 0);

    // 设置图优化的级别
    // ORT_DISABLE_ALL = 0,
    // ORT_ENABLE_BASIC = 1,  # 启用基础优化
    // ORT_ENABLE_EXTENDED = 2,
    // ORT_ENABLE_ALL = 99   # 启用所有可能的优化
    session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);

    // 创建通过ONNX模型路径进行实例化的Session
    // Ort::Session session(env, "model_path", session_options);
    // 通过ONNX模型的原始数据和模型大小
    session = Ort::Session(env, modelData, modelSize, session_options);

    // print model input layer (node names, types, shape etc.)
    Ort::AllocatorWithDefaultOptions allocator;

    // print number of model input nodes
    size_t num_input_nodes = session.GetInputCount();

    // 初始化输入张量的大小
    inputTensorSize = 10 * 20;
    inputTensorValues = std::vector<float>(inputTensorSize);

    for (unsigned int i = 0; i < inputTensorSize; i++) {
        inputTensorValues[i] = (float) i / (inputTensorSize + 1);
    }

    // create MemoryInfo
    memoryInfo = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);

}

int OnnxPredictor::doPredict(aura_cv::TensorArray &inputs, aura_cv::TensorArray &outputs) {


    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(memoryInfo, inputTensorValues.data(), inputTensorSize,
                                                              input_node_dims.data(), 2);
    assert(input_tensor.IsTensor());

    std::vector<int64_t> input_mask_node_dims = {1, 20, 4};
    size_t input_mask_tensor_size = 1 * 20 * 4;
    std::vector<float> input_mask_tensor_values(input_mask_tensor_size);
    for (unsigned int i = 0; i < input_mask_tensor_size; i++)
        input_mask_tensor_values[i] = (float) i / (input_mask_tensor_size + 1);
    // create input tensor object from data values
    auto mask_memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
    Ort::Value input_mask_tensor = Ort::Value::CreateTensor<float>(mask_memory_info, input_mask_tensor_values.data(),
                                                                   input_mask_tensor_size, input_mask_node_dims.data(),
                                                                   3);
    assert(input_mask_tensor.IsTensor());

    std::vector<Ort::Value> ort_inputs;
    ort_inputs.push_back(std::move(input_tensor));
    ort_inputs.push_back(std::move(input_mask_tensor));
    // score model & input tensor, get back output tensor
    auto output_tensors = session.Run(Ort::RunOptions{nullptr}, input_node_names.data(), ort_inputs.data(),
                                      ort_inputs.size(), output_node_names.data(), 2);

    // Get pointer to output tensor float values
    float *floatarr = output_tensors[0].GetTensorMutableData<float>();
    float *floatarr_mask = output_tensors[1].GetTensorMutableData<float>();

    printf("Done!\n");
    return 0;
}

int OnnxPredictor::deInit() {
    return 0;
}

}
```





