---
title:  Batch Normalization(BN)的基础学习
date: 2020-07-20 00:00:00
updated: 2022-01-05 00:00:00
tags: [Linux,四大组件,Activity]
type: [Linux,四大组件,Activity]
comments: Activity的生命周期完全解析
description: Activity的生命周期完全解析
keywords: Activity的生命周期完全解析
top_img:
mathjax:
katex:
aside:
aplayer:
highlight_shrink:
---

[TOC]

## 概述

一般来说，如果模型的输入特征不相关且满足标准正态分布N(0,1)的时候模型的表现一般较好。在训练神经网络模型时，我们可以事先将特征去相关并使得它们满足一个比较好的分布，这样，模型的第一层网络一般都会有一个比较好的输入特征，但是随着模型的层数加深，网络的非线性变换使得每一层的结果变得相关了，且不再满足N(0,1)分布。

更糟糕的是，可能这些隐藏层的特征分布已经发生了偏移。

论文的作者认为上面的问题会使得神经网络的训练变得困难，为了解决这个问题，他们提出在层与层之间加入Batch Normalization层。训练时，BN层利用隐藏层输出结果的均值![[公式]](images/equation-20220424113459862)与方差![[公式]](images/equation-20220424113459903)来标准化每一层特征的分布，并且维护所有mini-batch数据的均值与方差，最后利用样本的均值与方差的无偏估计量用于测试时使用。





## **BN层的作用**

贴出论文中的两张图，就可以说明BN层的作用

1. 使得模型训练收敛的速度更快
2. 模型隐藏输出特征的分布更稳定，更利于模型的学习